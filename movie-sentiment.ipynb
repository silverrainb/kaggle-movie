{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase(original)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhraseId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>A series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SentenceId                                             Phrase  \\\n",
       "PhraseId                                                                  \n",
       "1                  1  A series of escapades demonstrating the adage ...   \n",
       "2                  1  A series of escapades demonstrating the adage ...   \n",
       "3                  1                                           A series   \n",
       "4                  1                                                  A   \n",
       "5                  1                                             series   \n",
       "\n",
       "          Sentiment                                   Phrase(original)  \n",
       "PhraseId                                                                \n",
       "1                 1  A series of escapades demonstrating the adage ...  \n",
       "2                 2  A series of escapades demonstrating the adage ...  \n",
       "3                 2                                           A series  \n",
       "4                 2                                                  A  \n",
       "5                 2                                             series  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/train.tsv\", sep = \"\\t\", index_col = \"PhraseId\")\n",
    "train[\"Phrase(original)\"] = train[\"Phrase\"]\n",
    "print(train.shape)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66292, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase(original)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhraseId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156061</th>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156062</th>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156063</th>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156064</th>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156065</th>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SentenceId                                             Phrase  \\\n",
       "PhraseId                                                                  \n",
       "156061          8545  An intermittently pleasing but mostly routine ...   \n",
       "156062          8545  An intermittently pleasing but mostly routine ...   \n",
       "156063          8545                                                 An   \n",
       "156064          8545  intermittently pleasing but mostly routine effort   \n",
       "156065          8545         intermittently pleasing but mostly routine   \n",
       "\n",
       "                                           Phrase(original)  \n",
       "PhraseId                                                     \n",
       "156061    An intermittently pleasing but mostly routine ...  \n",
       "156062    An intermittently pleasing but mostly routine ...  \n",
       "156063                                                   An  \n",
       "156064    intermittently pleasing but mostly routine effort  \n",
       "156065           intermittently pleasing but mostly routine  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"./data/test.tsv\", sep = \"\\t\", index_col = \"PhraseId\")\n",
    "test[\"Phrase(original)\"] = test[\"Phrase\"]\n",
    "print(test.shape)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleantext -- 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK(Natural Language Toolkit) provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "Snowball is a small string processing language designed for creating stemming algorithms for use in Information Retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disappoint\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english') \n",
    "print(stemmer.stem(\"disappointments\")) #disappoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_phrase(phrase):\n",
    "    words = phrase.split(\" \")\n",
    "    stemmed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        stemmed_words.append(stemmed_word)\n",
    "\n",
    "    stemmed_phrase = \" \".join(stemmed_words)\n",
    "\n",
    "    return stemmed_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "Stemming... (train):  60%|██████    | 94063/156060 [00:07<00:05, 11199.41it/s]"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "tqdm.pandas(desc=\"Stemming... (train)\")\n",
    "\n",
    "train[\"Phrase\"] = train[\"Phrase\"].progress_apply(stem_phrase)\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "tqdm.pandas(desc = \"Stemming... (test)\")\n",
    "\n",
    "test[\"Phrase\"] = test[\"Phrase\"].progress_apply(stem_phrase)\n",
    "\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleantext 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "extra_stopwords = ['stopWord1','stopWord2']\n",
    "stopwords.extend(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud color\n",
    "def random_color_func(word=None, font_size=None, position=None,  \n",
    "                      orientation=None, font_path=None, random_state=None):\n",
    "    h = int(360.0 * float(random_state.randint(100, 150) / 255.0))\n",
    "    s = int(73)\n",
    "    l = int(48)\n",
    "\n",
    "    return \"hsl({}, {}%, {}%)\".format(h, s, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "wordcloud = WordCloud(stopwords = stopwords,\n",
    "                      background_color = 'white',\n",
    "                      width = 800, height = 600,\n",
    "                      color_func=random_color_func).generate(' '.join(train['Phrase']))\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleantext -- 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(phrase):\n",
    "    phrase = phrase.replace(\"n't\", \"not\")\n",
    "    phrase = phrase.replace(\"not\", \"no\")\n",
    "    phrase = phrase.replace(\"hopeless\", \"bad\")\n",
    "    phrase = phrase.replace(\"good\", \"best\")\n",
    "    phrase = phrase.replace(\"excellent\", \"best\")\n",
    "    phrase = phrase.replace(\"funni\", \"fun\")\n",
    "    phrase = phrase.replace(\"funny\", \"fun\")\n",
    "    phrase = phrase.replace(\"littl\", \"little\")\n",
    "    phrase = phrase.replace(\"the movi\", \"movie\")\n",
    "    phrase = phrase.replace(\"veri\", \"very\")\n",
    "    phrase = phrase.replace(\"onli\", \"only\")\n",
    "    phrase = phrase.replace(\"comedi\", \"comedy\")\n",
    "    phrase = phrase.replace(\"veri\", \"very\")\n",
    "    phrase = phrase.replace(\"stori\", \"story\")\n",
    "    phrase = phrase.replace(\"charact\", \"character\")\n",
    "\n",
    "    return phrase\n",
    "\n",
    "train[\"Phrase\"] = train[\"Phrase\"].apply(clean_text)\n",
    "test[\"Phrase\"] = test[\"Phrase\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "wordcloud = WordCloud(stopwords = stopwords,\n",
    "                      background_color = 'white',\n",
    "                      width = 800, height = 600,\n",
    "                      color_func=random_color_func).generate(' '.join(train['Phrase']))\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.axis(\"off\") # 축에 표시되는 눈금을 제거하는 옵션\n",
    "plt.imshow(wordcloud) # 이미지가 표시되도록 하는 옵션.\n",
    "plt.show() # 최종으로 보여주는 옵션."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one Hot Encode Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**\n",
    "$$ \\frac{TF}{IDF} $$\n",
    "\n",
    "* Term frequency  (per sentence)\n",
    "* Document frequency  (per document)\n",
    "\n",
    "values uniqueness 1 > 0 (a,the,etc..)\n",
    "* tree(countVectorizer)\n",
    "* regression(TFIDF)\n",
    "\n",
    "When it comes to NLP, even if it is just one thing that changed, tuning has to be re-done such as learning rate alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features = 30000, ngram_range = (1,4), stop_words =['the',\n",
    "'of','to','it','in','that','an','of the','this','his','about','at','or','than','from','in the','are','so','rrb',\n",
    "'the film','who','lrb','to the','doe','do','for the','director','been','ani','on the'])\n",
    "\n",
    "vectorizer.fit(train[\"Phrase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(train[\"Phrase\"])\n",
    "# applies the params from fiting to data\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = vectorizer.get_feature_names()# <- vocab\n",
    "# \"not funny\" in columns\n",
    "\n",
    "train_vector = pd.DataFrame(X_train.toarray(), columns= columns)\n",
    "print(train_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "# This takes a while\n",
    "# This will take each train dataset's word and count the frequency in dictionary form.\n",
    "for column in train_vector.columns:\n",
    "    dictionary[column] = train_vector[column].sum()\n",
    "\n",
    "dictionary_dataframe = pd.DataFrame.from_dict({'word': list(dictionary.keys()), 'count': list(dictionary.values())})\n",
    "dictionary_dataframe = dictionary_dataframe.sort_values('count', ascending=False)\n",
    "dictionary_dataframe = dictionary_dataframe[0:100]\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "figure, ax = plt.subplots(nrows=1, ncols=1)\n",
    "figure.set_size_inches(18, 64)\n",
    "\n",
    "sns.barplot(data=dictionary_dataframe, y=\"word\", x=\"count\", ax=ax)\n",
    "# dictionary_dataframe.to_csv(\"frequency.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(analyzer = 'char', \n",
    "                                  max_features = 10000, \n",
    "                                  ngram_range = (1,9))\n",
    "\n",
    "char_vectorizer.fit(train[\"Phrase\"])\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(analyzer = 'word', \n",
    "                                  max_features = 30000, \n",
    "                                  ngram_range = (1,4))\n",
    "\n",
    "word_vectorizer.fit(train[\"Phrase\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting finds the internal parameters of a model that will be used to transform data.\n",
    "\n",
    "Transforming applies the parameters to data.\n",
    "\n",
    "You may fit a model to one set of data, and then transform it on a completely different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word = word_vectorizer.transform(train[\"Phrase\"])\n",
    "print(X_train_word.shape)\n",
    "X_train_char = char_vectorizer.transform(train[\"Phrase\"])\n",
    "print(X_train_char.shape)\n",
    "\n",
    "from scipy.sparse import hstack #vstack, hstack (word, character to merge )\n",
    "X_train = hstack([X_train_char, X_train_word])\n",
    "print(X_train.shape)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_word = word_vectorizer.transform(test[\"Phrase\"])\n",
    "print(X_test_word.shape)\n",
    "X_test_char = char_vectorizer.transform(test[\"Phrase\"])\n",
    "print(X_test_char.shape)\n",
    "\n",
    "X_test = hstack([X_test_char, X_test_word])\n",
    "print(X_test.shape)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[\"Sentiment\"]\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "seed = 23\n",
    "\n",
    "model = SGDClassifier(n_jobs=-1,\n",
    "                      alpha = 0.00005,\n",
    "                      random_state=seed)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = cross_val_predict(model, X_train, y_train, cv = 5)\n",
    "\n",
    "score = accuracy_score(y_train, predictions)\n",
    "\n",
    "print(f\"Score = {score.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GroupKFold\n",
    "# from sklearn.cross_validation import cross_val_score, GroupKFold\n",
    "\n",
    "kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "score = cross_val_score(model, X_train, y_train, cv=kfold, groups=train[\"SentenceId\"]).mean()\n",
    "\n",
    "print(\"Score = {0:.5f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'booster': 'gblinear',\n",
    "    'objective': 'multi:softmax',\n",
    "    'lambda': 2.186753e-03,\n",
    "    'alpha': 1.286904,\n",
    "    'lambda_bias': 6.191707e+00,\n",
    "    'num_class': 5,\n",
    "    'nthread':2,\n",
    "}\n",
    "\n",
    "booster = xgb.train(params, dtrain, num_boost_round=98)\n",
    "booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(X_test.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = booster.predict(dtest)\n",
    "print(predictions.shape)\n",
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./data/sampleSubmission.csv\", index_col=\"PhraseId\")\n",
    "\n",
    "submission[\"Sentiment\"] = predictions\n",
    "\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now()\n",
    "current_date = current_date.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "desc = \"\"\n",
    "sign = \"silverrain\"\n",
    "\n",
    "filename = \"{date}_{score:.5f}_{desc}_{sign}.csv\".format(date=current_date, \n",
    "                                                         score=score, \n",
    "                                                         desc=desc, \n",
    "                                                         sign=sign)\n",
    "# filename = f\"{date}_{score}_{desc}_{sign}.csv\"\n",
    "filepath = \"{filename}\".format(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
